<html>
<head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<title>机器学习Coursera-ML | 小虾米的记录</title>

<link rel="shortcut icon" href="https://xmweijh.github.io//favicon.ico?v=1626350788642">

<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://xmweijh.github.io//styles/main.css">
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css"> -->

<style>
    hr {
        margin-top: 1rem;
        margin-bottom: 1rem;
        border: 0;
        border-top: 1px solid rgba(0, 0, 0, 0.1);
    }
</style>

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>

<!-- <script src="https://cdn.jsdelivr.net/npm/moment@2.27.0/moment.min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js"></script> -->
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <style>
    /* 导航栏样式 */
    .navbar {
        position: relative;
        display: -ms-flexbox;
        display: flex;
        -ms-flex-wrap: wrap;
        flex-wrap: wrap;
        -ms-flex-align: center;
        align-items: center;
        -ms-flex-pack: justify;
        justify-content: space-between;
        padding: 0.5rem 1rem;
    }

    .navbar-brand {
        display: inline-block;
        padding-top: 0.3125rem;
        padding-bottom: 0.3125rem;
        margin-right: 1rem;
        font-size: 1.25rem;
        line-height: inherit;
        white-space: nowrap;
    }

    .navbar-brand:hover,
    .navbar-brand:focus {
        text-decoration: none;
    }

    .navbar-nav {
        display: -ms-flexbox;
        display: flex;
        -ms-flex-direction: column;
        flex-direction: column;
        padding-left: 0;
        margin-bottom: 0;
        list-style: none;
    }

    .navbar-collapse {
        -ms-flex-preferred-size: 100%;
        flex-basis: 100%;
        -ms-flex-positive: 1;
        flex-grow: 1;
        -ms-flex-align: center;
        align-items: center;
    }

    .navbar-toggler {
        padding: 0.25rem 0.75rem;
        font-size: 1.25rem;
        line-height: 1;
        background-color: transparent;
        border: 1px solid transparent;
        border-radius: 0.25rem;
    }

    .navbar-toggler:hover,
    .navbar-toggler:focus {
        text-decoration: none;
    }

    @media (min-width: 992px) {
        .navbar-expand-lg {
            -ms-flex-flow: row nowrap;
            flex-flow: row nowrap;
            -ms-flex-pack: start;
            justify-content: flex-start;
        }

        .navbar-expand-lg .navbar-nav {
            -ms-flex-direction: row;
            flex-direction: row;
        }

        .navbar-expand-lg .navbar-collapse {
            display: -ms-flexbox !important;
            display: flex !important;
            -ms-flex-preferred-size: auto;
            flex-basis: auto;
        }

        .navbar-expand-lg .navbar-toggler {
            display: none;
        }
    }

    @media (max-width: 991px) {
        #navbarSupportedContent {
            display: none;
        }
    }
</style>
<nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            小虾米的记录
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
            <div class="nav-item">
                
                <a href="/" class="menu gt-a-link">
                    首页
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/archives" class="menu gt-a-link">
                    归档
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/tags" class="menu gt-a-link">
                    标签
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/post/about" class="menu gt-a-link">
                    关于
                </a>
                
            </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1626350788642"
                action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>
<script>
    /* 移动端导航栏展开/收起切换 */
    document.getElementById('changeNavbar').onclick = function () {
        var element = document.getElementById('navbarSupportedContent');
        if (element.style.display === 'none' || element.style.display === '') {
            element.style.display = 'block';
        } else {
            element.style.display = 'none';
        }
    }
</script>
    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    机器学习Coursera-ML
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2021-06-26 ·
                    </time>
                    
                        <a href="https://xmweijh.github.io/tag/xHTgICqZ2/" class="post-tags">
                            # Python
                        </a>
                    
                </div>
                <div class="post-content">
                    <h1 id="机器学习coursera-ml">机器学习Coursera-ML</h1>
<h3 id="定义">定义：</h3>
<p>第一个机器学习的定义来自于<strong>Arthur Samuel</strong>。他定义机器学习为，在进行特定编程的情况下，给予计算机学习能力的领域。</p>
<p>另一个定义，由<strong>Tom Mitchell</strong>提出，<strong>Tom</strong>定义的机器学习是，一个好的学习问题定义如下，他说，一个程序被认为能从经验<strong>E</strong>中学习，解决任务<strong>T</strong>，达到性能度量值<strong>P</strong>，当且仅当，有了经验<strong>E</strong>后，经过<strong>P</strong>评判，程序在处理T时的性能有所提升。</p>
<h3 id="监督学习">监督学习</h3>
<p>监督学习指的就是我们给学习算法一个数据集。</p>
<p>回归问题（连续）   分类问题（离散）</p>
<p><strong>支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征</strong></p>
<h3 id="无监督学习">无监督学习</h3>
<p>不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。</p>
<h2 id="单变量线性回归">单变量线性回归</h2>
<p>只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>
<p>型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（<strong>modeling error</strong>）</p>
<p>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 最小。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>梯度下降是一个用来求函数最小值的算法</p>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>
<pre><code class="language-python">def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
</code></pre>
<p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p>
<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>正规方程</p>
<figure data-type="image" tabindex="1"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210614112326242-1623641023485.png" alt="image-20210614112326242" loading="lazy"></figure>
<p>只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<pre><code class="language-python">import numpy as np
    
 def normalEqn(X, y):
    
   theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X等价于X.T.dot(X)
    
   return theta
</code></pre>
<h2 id="逻辑回归logistic-regression">逻辑回归(Logistic Regression)</h2>
<p>线性回归算法不适合于来解决一个分类问题，而适合于回归问题</p>
<blockquote>
<p>分类和回归的区别在于输出变量的类型。<br>
定量输出称为回归，或者说是连续变量预测；<br>
定性输出称为分类，或者说是离散变量预测。</p>
</blockquote>
<p>而逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y取值离散的情况，如：1 0 0 1。</p>
<figure data-type="image" tabindex="2"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210615151842853.png" alt="image-20210615151842853" loading="lazy"></figure>
<pre><code class="language-python">import numpy as np
    
def sigmoid(z):
    
   return 1 / (1 + np.exp(-z))
</code></pre>
<p>决策边界(<strong>decision boundary</strong>)</p>
<figure data-type="image" tabindex="3"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210615154450765.png" alt="image-20210615154450765" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210615154609670.png" alt="image-20210615154609670" loading="lazy"></figure>
<pre><code class="language-python">import numpy as np
    
def cost(theta, X, y):
    
  theta = np.matrix(theta)
  X = np.matrix(X)
  y = np.matrix(y)
  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
  second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
  return np.sum(first - second) / (len(X))
</code></pre>
<p><strong>共轭梯度法 BFGS</strong> (<strong>变尺度法</strong>) 和<strong>L-BFGS</strong> (<strong>限制变尺度法</strong>)</p>
<h2 id="正则化regularization">正则化(Regularization)</h2>
<p>如果我们发现了过拟合问题，应该如何处理？</p>
<ol>
<li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</li>
<li>正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</li>
</ol>
<pre><code class="language-python">import numpy as np
​
def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg
</code></pre>
<h2 id="神经网络">神经网络</h2>
<p>无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p>
<p>使用神经网络时的步骤：</p>
<p>网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</p>
<p>第一层的单元数即我们训练集的特征数量。</p>
<p>最后一层的单元数是我们训练集的结果的类的数量。</p>
<p>如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</p>
<p>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p>
<p>训练神经网络：</p>
<ol>
<li>参数的随机初始化</li>
<li>利用正向传播方法计算所有的hx</li>
<li>编写计算代价函数 的代码</li>
<li>利用反向传播方法计算所有偏导数</li>
<li>利用数值检验方法检验这些偏导数</li>
<li>使用优化算法来最小化代价函数</li>
</ol>
<h2 id="应用机器学习的建议advice-for-applying-machine-learning">应用机器学习的建议(Advice for Applying Machine Learning)</h2>
<pre><code>当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？
</code></pre>
<ol>
<li>
<p>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</p>
</li>
<li>
<p>尝试减少特征的数量</p>
</li>
<li>
<p>尝试获得更多的特征</p>
</li>
<li>
<p>尝试增加多项式特征</p>
</li>
<li>
<p>尝试减少正则化程度</p>
</li>
<li>
<p>尝试增加正则化程度</p>
<pre><code> 我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。
</code></pre>
</li>
</ol>
<p>它们也被称为&quot;机器学习诊断法&quot;。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。</p>
<pre><code>为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。
</code></pre>
<h3 id="模型选择和交叉验证集">模型选择和交叉验证集</h3>
<p>模型选择的方法为：</p>
<ol>
<li>使用训练集训练出10个模型</li>
<li>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li>
<li>选取代价函数值最小的模型</li>
<li>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</li>
</ol>
<pre><code>高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。

训练集误差和交叉验证集误差近似时：偏差/欠拟合
​
交叉验证集误差远大于训练集误差时：方差/过拟合
</code></pre>
<p>选择λ的方法为：</p>
<ol>
<li>使用训练集训练出12个不同程度正则化的模型</li>
<li>用12个模型分别对交叉验证集计算的出交叉验证误差</li>
<li>选择得出交叉验证误差<strong>最小</strong>的模型</li>
<li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上</li>
</ol>
<pre><code>• 当λ较小时，训练集误差较小（过拟合）而交叉验证集误差较大• 随着λ的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。
</code></pre>
<ol>
<li>获得更多的训练样本——解决高方差</li>
<li>尝试减少特征的数量——解决高方差</li>
<li>尝试获得更多的特征——解决高偏差</li>
<li>尝试增加多项式特征——解决高偏差</li>
<li>尝试减少正则化程度λ——解决高偏差</li>
<li>尝试增加正则化程度λ——解决高方差</li>
</ol>
<pre><code>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。

通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。

对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，然后选择交叉验证集代价最小的神经网络。
</code></pre>
<h2 id="机器学习系统的设计">机器学习系统的设计</h2>
<p>构建一个学习算法的推荐方法为：</p>
<ol>
<li>从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</li>
<li>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</li>
<li>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势</li>
</ol>
<p>偏斜类（skewed classes）的问题。类偏斜情况表现为我们的训练集中有非常多的同一种类的样本，只有很少或没有其他类的样本。</p>
<h3 id="查准率和查全率之间的权衡">查准率和查全率之间的权衡</h3>
<figure data-type="image" tabindex="5"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210621173328942.png" alt="image-20210621173328942" loading="lazy"></figure>
<h2 id="支持向量机support-vector-machines">支持向量机(Support Vector Machines)</h2>
<figure data-type="image" tabindex="6"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210623090847215.png" alt="image-20210623090847215" loading="lazy"></figure>
<p>支持向量机有时被称为<strong>大间距分类器</strong></p>
<p>较大时，相当于λ 较小，可能会导致过拟合，高方差。</p>
<p>较小时，相当于λ 较大，可能会导致低拟合，高偏差。</p>
<figure data-type="image" tabindex="7"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210623104225022.png" alt="image-20210623104225022" loading="lazy"></figure>
<p>σ较大时，可能会导致低方差，高偏差；</p>
<p>σ较小时，可能会导致低偏差，高方差。</p>
<h2 id="聚类clustering">聚类(Clustering)</h2>
<p><strong>K-均值</strong>是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</p>
<p><strong>K-均值</strong>是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为:</p>
<p>首先选择K个随机的点，称为<strong>聚类中心</strong>（<strong>cluster centroids</strong>）；</p>
<p>对于数据集中的每一个数据，按照距离个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</p>
<p>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</p>
<p>重复步骤2-4直至中心点不再变化。</p>
<figure data-type="image" tabindex="8"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210624092652680.png" alt="image-20210624092652680" loading="lazy"></figure>
<p>在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做:</p>
<p>1.我们应该选择K&lt;m，即聚类中心点的个数要小于所有训练集实例的数量<br>
2.随机选择K个训练实例，然后令K个聚类中心分别与这K个训练实例相等</p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用<strong>K-均值</strong>算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p>
<h2 id="降维dimensionality-reduction">降维(Dimensionality Reduction)</h2>
<h3 id="动机一数据压缩">动机一：数据压缩</h3>
<h3 id="动机二数据可视化">动机二：数据可视化</h3>
<h3 id="主成分分析问题">主成分分析问题</h3>
<p>主成分分析(<strong>PCA</strong>)是最常见的降维算法。</p>
<p>在<strong>PCA</strong>中，我们要做的是找到一个方向向量（<strong>Vector direction</strong>），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p>
<figure data-type="image" tabindex="9"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210624105033999.png" alt="image-20210624105033999" loading="lazy"></figure>
<p>主要成分分析是减少投射的平均均方误差：</p>
<p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的k值。</p>
<figure data-type="image" tabindex="10"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210624112150628.png" alt="image-20210624112150628" loading="lazy"></figure>
<p>错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p>
<p>另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>
<h2 id="异常检测anomaly-detection">异常检测(Anomaly Detection)</h2>
<figure data-type="image" tabindex="11"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210625095319096.png" alt="image-20210625095319096" loading="lazy"></figure>
<p>之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测：</p>
<p>两者比较：</p>
<table>
<thead>
<tr>
<th style="text-align:left">异常检测</th>
<th style="text-align:left">监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">非常少量的正向类（异常数据 y=1）, 大量的负向类（y=0）</td>
<td style="text-align:left">同时有大量的正向类和负向类</td>
</tr>
<tr>
<td style="text-align:left">许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。</td>
<td style="text-align:left">有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。</td>
</tr>
<tr>
<td style="text-align:left">未来遇到的异常可能与已掌握的异常、非常的不同。</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况</td>
<td style="text-align:left">例如：邮件过滤器 天气预报 肿瘤分类</td>
</tr>
</tbody>
</table>
<p>原高斯分布模型和多元高斯分布模型的比较：</p>
<table>
<thead>
<tr>
<th style="text-align:left">原高斯分布模型</th>
<th style="text-align:left">多元高斯分布模型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决</td>
<td style="text-align:left">自动捕捉特征之间的相关性</td>
</tr>
<tr>
<td style="text-align:left">计算代价低，能适应大规模的特征</td>
<td style="text-align:left">计算代价较高 训练集较小时也同样适用</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">必须要有 m&gt;n，不然的话协方差矩阵 不可逆的，通常需要 m&gt;10n 另外特征冗余也会导致协方差矩阵不可逆</td>
</tr>
</tbody>
</table>
<figure data-type="image" tabindex="12"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210625114024157.png" alt="image-20210625114024157" loading="lazy"></figure>
<h3 id="协同过滤算法">协同过滤算法<img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210626160916302.png" alt="image-20210626160916302" loading="lazy"></h3>
<h3 id="随机梯度下降法">随机梯度下降法</h3>
<figure data-type="image" tabindex="13"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210626162433415.png" alt="image-20210626162433415" loading="lazy"></figure>
<h3 id="小批量梯度下降">小批量梯度下降</h3>
<figure data-type="image" tabindex="14"><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20210626162447571.png" alt="image-20210626162447571" loading="lazy"></figure>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://xmweijh.github.io/post/python-xiao-ji/" class="post-title gt-a-link">
                    Python小记
                </a>
            </div>
        

        

        

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">努力成就自我</div>
    <div class="social-container">
        
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://xmweijh.github.io//atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
